# FinalYearProjectV2
# To run localy
# Install LM Studio
# On LM Studio you must download llama-3.2-1b-instruct
# Run the model localy and ensure the reachable adtress matches for me its http://127.0.0.1:1234
# You can change the adress in the SentimentLLM.py. This will be the base_url in client
# Once this is done it should all word seamlessly.

# ---Problems you may encounter---
# Ensure you look at the requirements.txt for dependencies
# If the LLM Stops working you may need to increase the context length in LM Studio 
